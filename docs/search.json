[
  {
    "objectID": "posts/2022-11-05-the-data-monitoring-system/index.html",
    "href": "posts/2022-11-05-the-data-monitoring-system/index.html",
    "title": "shriyan.io",
    "section": "",
    "text": "The Data You Know; The Story You Don’t\n\n\ntoc: true\nbadges: true\ncomments: true\ncategories: [jupyter]\nimage: images/chart-preview.png\n\n\n\nWhen you deal with data coming into your system at regular intervals(hourly, daily, weekly etc.), there are few things you will need to handle in such a scenario: 1. Schedule a task which can download, preprocess and push the data to database at regular interval. 2. Check for data sanity and visualize the same for better understanding of the data. 3. In case of discrepancy observed, report the same to stakeholders or concerned team.\nThe idea to develop such a monitoring system, which keeps everyone updated about the nature of data received and also the discrepancy in it, is crucial to any data science pipeline. In this blog post we will further expand on this idea and demonstrate how to go about setting up such a system with the only constraint being, that the data should be available at certain fixed interval in a time series fashion.\n\n\n\n\n\n\n\n\n\ndocker pull postgres\ndocker run --name <name_of_container> -e POSTGRES_USER=<username> -e POSTGRES_PASSWORD=<password> -p 5432:5432 -v /data:/var/lib/postgresql/data -d <db_name>\n\n\n\nRun the below instructions inside the repo where your your python code resides.\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.4.1/docker-compose.yaml'\nmkdir -p ./dags ./logs ./plugins\necho -e 'AIRFLOW_UID=$(id -u)' > .env\ndocker-compose up airflow-init\n\n\n\ngit clone https://github.com/apache/superset.git\ncd superset\ndocker-compose -f docker-compose-non-dev.yml pull\ndocker-compose -f docker-compose-non-dev.yml up\n\n\n\n\nAccording to official documentation: > Apache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows.\nThe below steps ensure that you have an up and running job scheduled at regular intervals:\n\nDefine a function to fetch data from the API or from S3 or any other source\n\ndef download_api_data():\n    print(f\"Fetching data....\")\n    response = requests.get(<API url>)\n    data = response.json()\n    print(f\"Total number of data: {len(data)}\")\n    json_object = json.dumps(data, indent=2)\n    with open(f\"/tmp/pdl_{currency}_hourly.json\", \"w\") as f:\n        f.write(json_object)\n    print(f\"Finished downloading data.....\")\nThe above function fetches data from the API and then stores it as json file for further processing.\n\nDefine a function to move downloaded data(json, csv etc.) to PostgreSQL\n\ndef move_pdl_data_to_postgres(**kwargs):\n    currency = kwargs[\"currency\"]\n    print(f\"Populating for {currency} has started\")\n    with open(f\"/tmp/pdl_{currency}_hourly.json\") as f:\n        data = json.load(f)\n    df = pd.DataFrame(data)\n\n    # Define your preprocessing steps here like typecasting a column according to the Postgresql schema and any other steps specific to your use case\n\n    print(\"All values created, starting the push to db\")\n    df.to_sql(name=\"<name-of-sql-table>\", con=engine, index=False, if_exists=\"append\", chunksize=300)\nIn the above function we basically load the json data downloaded in step 1 inside a dataframe and then move it to PostgreSQL table, defined in our con parameter of to_sql function.\n\nDefine a DAG\n\n\nDAG object is needed to define how we are going to schedule our various tasks.\nHere we pass a string that defines the dag_id, which serves as a unique identifier for your DAG and also description.\nWe also schedule a DAG using schedule_interval parameter to run it at hourly frequency and also provide start_date of the DAG.\nBelow is an example of the DAG definition:\n\ndag = DAG(\n    dag_id=\"data-migration\",\n    description=\"Daily data monitoring pipeline\",\n    schedule_interval=\"0 * * * *\",\n    start_date=datetime(2022, 10, 17),\n)\n\nDefine PythonOperator\n\n\nOperators are tasks that define a unit of work to be done.\nThere are manny different kind of operators that you can play around with in Airflow. But we will stick to PythonOperator, which takes python function as a parameter.\nHere we define the task_id, python_callable and above defined dag object.\nBelow is how we define the PythonOperator object\n\nPythonOperator(\n            task_id=f\"download_json\",\n            python_callable=download_json,\n            dag=dag,\n        )\n\nSetup task dependencies\n\nLets say we have 2 PythonOperator defined as 2 tasks and one task is dependent on the other. In our case we first fetch the data from API and then push the data to PostgreSQL. So setting up task dependency kind of becomes and it is defined by using >> operator as follows:\ntask1 >> task2\nHere the Airflow DAG knows that it has to first finish running the task1 and then move on to task2. Any failure in task1 will result in termination of the job.\n\n\n\n\nThe above UI can be accessed after Airflow login and navigating as follows: <DAG-name> > Graph.\nThe Graph shows you the various tasks that are scheduled to run and each row defines multiple tasks and how each one is dependent on the other i.e Task move_pdl_data_to_postgres_ADA is dependent on download_pdl_json_ADA and hence has to be completed first.\nThe subsequent rows follow a similar pattern and here we have demonstrated multiple different jobs scheduled inside a single DAG, where each job does the same thing as other, but for different type of data i.e for different bitcoin currencies in our scenario.\n\n\n\n\n\nSuperset is a data exploration and visualization platform and we are going to leverage it to use it as our frontend for monitoring the data we move to the PostgreSQL at regular intervals.\nAs seen in the above example dashboard we are doing some sanity check and checking the trend for a bitcoin currency.\nSo playing around with visualizations specific to your data and problem statement is straight forward in Superset and it comes with a bunch of features.\n\n\n\n\nThis task can be further expanded in various aspects each from PostgreSQl, Airflow and Superset perspective, by adding more sources of information that we want to monitor in real time and keep adding more tables to our PostgreSQL database, schedule more DAGs in our Airflow container and add more dashboards monitoring the nature of different data."
  },
  {
    "objectID": "posts/2022-09-14-the-synthetic-data-story/index.html",
    "href": "posts/2022-09-14-the-synthetic-data-story/index.html",
    "title": "shriyan.io",
    "section": "",
    "text": "The Data You Know; The Story You Don’t\n\n\ntoc: true\nbadges: true\ncomments: true\ncategories: [jupyter]\nimage: images/chart-preview.png"
  },
  {
    "objectID": "posts/2022-09-14-the-synthetic-data-story/index.html#what-is-sdv",
    "href": "posts/2022-09-14-the-synthetic-data-story/index.html#what-is-sdv",
    "title": "shriyan.io",
    "section": "What is SDV?",
    "text": "What is SDV?\nThe Synthetic Data Vault (SDV) is a Synthetic Data Generation ecosystem of libraries that allows users to generate new Synthetic Data that has the same format and statistical properties as the original dataset.\nThe library can also cater to different nature of data as below: 1. single-table - Used to model single table datasets. 2. multi-table - Used to model relational datasets. 3. timeseries - Used to model time-series datasets.\nFeature Highlights\n\nSynthetic data generators for single tables with the following features:\n\nUsing Copulas and Deep Learning based models.\nHandling of multiple data types and missing data with minimum user input.\nSupport for pre-defined and custom constraints and data validation.\n\nSynthetic data generators for complex multi-table, relational datasets with the following features:\n\nDefinition of entire multi-table datasets metadata with a custom and flexible JSON schema.\nUsing Copulas and recursive modeling techniques.\n\nSynthetic data generators for multi-type, multi-variate timeseries with the following features:\n\nUsing statistical, Autoregressive and Deep Learning models.\nConditional sampling based on contextual attributes."
  },
  {
    "objectID": "posts/2022-09-14-the-synthetic-data-story/index.html#getting-started-with-sdv",
    "href": "posts/2022-09-14-the-synthetic-data-story/index.html#getting-started-with-sdv",
    "title": "shriyan.io",
    "section": "Getting started with SDV",
    "text": "Getting started with SDV\npip install sdv"
  },
  {
    "objectID": "posts/2022-09-14-the-synthetic-data-story/index.html#code",
    "href": "posts/2022-09-14-the-synthetic-data-story/index.html#code",
    "title": "shriyan.io",
    "section": "Code",
    "text": "Code\n\nimport pandas as pd\nfrom sdv.timeseries import PAR\nimport altair as alt\n\n/home/sharad/repo/projects/data-glance/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nLoad synthetic data\n\n\ndf = pd.read_csv(\"../data/synthetic.csv\")\ndf.loc[:, \"date\"] = pd.to_datetime(df.date)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      websiteName\n      category\n      country\n      state\n      city\n      os\n      device\n      product\n      date\n      count\n    \n  \n  \n    \n      0\n      okaz.com.sa\n      society&culture\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      2022-05-01 00:00:00\n      545\n    \n    \n      1\n      okaz.com.sa\n      society&culture\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      2022-05-01 01:00:00\n      884\n    \n    \n      2\n      okaz.com.sa\n      society&culture\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      2022-05-01 02:00:00\n      708\n    \n    \n      3\n      okaz.com.sa\n      society&culture\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      2022-05-01 03:00:00\n      550\n    \n    \n      4\n      okaz.com.sa\n      society&culture\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      2022-05-01 04:00:00\n      271\n    \n  \n\n\n\n\n\nLets understand how we go about defining our auto-regressive time series model to model our data. There are 2 key parameters that would specifically define how our model is going to learn from our time series data.\nAs mentioned before about the heirarchical nature of the data, we would define our PAR model parameters according to that:\n\nentity_columns: This uniquely defines the entity for which we want to generate a time series. In our case the entity is not singular i.e we want time series for each unique group comprising of websiteName, category, country, state, city, os, device, product.\nsequence_index: Datetime column for time series reference.\n\n\n\nentity_columns = ['category', 'country', 'state', 'city', 'os', 'device', 'product', 'websiteName']\nsequence_index = 'date'\n\nmodel = PAR(\n            entity_columns=entity_columns,\n            sequence_index=sequence_index,\n           )\n\nFit PAR model on the original data\n\n%%time\nmodel.fit(df)\n\nCPU times: user 28.4 s, sys: 154 ms, total: 28.5 s\nWall time: 11 s\n\n\nConditional Sampling\n\nThere is one key feature that PAR model provides, which we are going to specifically use for our problem, that is instead of generating random samples from the model based on raw data, we can provide specific context in which we want the samples.\nThis can be achieved by defining a dataframe consisting of unique entities for which we want to generate a time-series for. One thing to note here is that, the raw data we provided to the model, has 24 hours of data for each unique entity. So the multiple contexts that we define in the next step, the model is going to output 24 rows for each entity.\n\n\ncontext_columns = ['category', 'country', 'state', 'city', 'os', 'device', 'product', 'websiteName']\ndf_freq = df.groupby(context_columns).date.count().sort_values(ascending=False).reset_index()\nmost_freq_group = df_freq[context_columns].to_dict(orient=\"records\")\n\ncontext = pd.DataFrame(\n   most_freq_group\n)\n\n\ncontext\n\n\n\n\n\n  \n    \n      \n      category\n      country\n      state\n      city\n      os\n      device\n      product\n      websiteName\n    \n  \n  \n    \n      0\n      arts&entertainment\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      okaz.com.sa\n    \n    \n      1\n      arts&entertainment\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      vibe\n      okaz.com.sa\n    \n    \n      2\n      society&culture\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      impulse\n      okaz.com.sa\n    \n    \n      3\n      society&culture\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      RotatingCube\n      okaz.com.sa\n    \n    \n      4\n      society&culture\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      vibe\n      okaz.com.sa\n    \n    \n      5\n      society&culture\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      impulse\n      okaz.com.sa\n    \n    \n      6\n      society&culture\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      okaz.com.sa\n    \n    \n      7\n      news\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      vibe\n      okaz.com.sa\n    \n    \n      8\n      news\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      impulse\n      okaz.com.sa\n    \n    \n      9\n      news\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      RotatingCube\n      okaz.com.sa\n    \n    \n      10\n      news\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      vibe\n      okaz.com.sa\n    \n    \n      11\n      news\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      impulse\n      okaz.com.sa\n    \n    \n      12\n      news\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      okaz.com.sa\n    \n    \n      13\n      health&fitness\n      indonesia\n      jawa barat\n      bekasi\n      Android\n      mobile\n      impulse\n      sajiansedap.grid.id\n    \n    \n      14\n      business&finance\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      RotatingCube\n      okaz.com.sa\n    \n    \n      15\n      business&finance\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      vibe\n      okaz.com.sa\n    \n    \n      16\n      business&finance\n      saudi arabia\n      ar riyad\n      riyadh\n      iOS\n      mobile\n      RotatingCube\n      okaz.com.sa\n    \n    \n      17\n      arts&entertainment\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      vibe\n      okaz.com.sa\n    \n    \n      18\n      arts&entertainment\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      RotatingCube\n      okaz.com.sa\n    \n    \n      19\n      society&culture\n      saudi arabia\n      makkah al mukarramah\n      jeddah\n      iOS\n      mobile\n      vibe\n      okaz.com.sa\n    \n  \n\n\n\n\nGenerate synthetic data\n\ndf_synthesized = model.sample(context=context)\n\n\ndf_synthesized.shape\n\n(480, 10)\n\n\n\nAs we passed 20 contexts to the model and we were expecting 24 rows for each context, the size of synthesize data is therfore 20 * 24 = 480\n\n\ndf_synthesized.loc[:, \"group_id\"] = df_synthesized.groupby(['category', 'country', 'state', 'city', 'os', 'device', 'product', 'websiteName']).ngroup().apply(lambda x: str(x) + \"_syn\")\n\ndf.loc[:, \"group_id\"] = df.groupby(['category', 'country', 'state', 'city', 'os', 'device', 'product', 'websiteName']).ngroup().apply(lambda x: str(x) + \"_og\")\n\n\nFor the purpose ofo visualizing the raw and model generated data, we create unique group-ids for both raw(groupid_og) and synthetic(groupid_syn) data\n\n\ndef plot_timeseries(df):\n    # select a point for which to provide details-on-demand\n    label = alt.selection_single(\n        encodings=['x'], # limit selection to x-axis value\n        on='mouseover',  # select on mouseover events\n        nearest=True,    # select data point nearest the cursor\n        empty='none'     # empty selection includes no data points\n    )\n\n    # define our base line chart of stock prices\n    base = alt.Chart(df).mark_line().encode(\n        alt.X('date:T'),\n        alt.Y('count:Q'),\n        alt.Color('group_id:N')\n    )\n\n    return alt.layer(\n        base, # base line chart\n\n        # add a rule mark to serve as a guide line\n        alt.Chart().mark_rule(color='#aaa').encode(\n            x='date:T'\n        ).transform_filter(label),\n\n        # add circle marks for selected time points, hide unselected points\n        base.mark_circle().encode(\n            opacity=alt.condition(label, alt.value(1), alt.value(0))\n        ).add_selection(label),\n\n        # add white stroked text to provide a legible background for labels\n        base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n            text='count:Q'\n        ).transform_filter(label),\n\n        # add text labels for stock prices\n        base.mark_text(align='left', dx=5, dy=-5).encode(\n            text='count:Q'\n        ).transform_filter(label),\n\n        data=df\n    ).properties(\n        width=500,\n        height=400\n    )\n\nGroupwise OG vs Synthetic time-series\n\ndf_vis = pd.concat([df[df.group_id.isin([\"0_og\"])], df_synthesized[df_synthesized.group_id.isin([\"0_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)\n\n\n\n\n\n\n\ndf_vis = pd.concat([df[df.group_id.isin([\"1_og\"])], df_synthesized[df_synthesized.group_id.isin([\"1_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)\n\n\n\n\n\n\n\ndf_vis = pd.concat([df[df.group_id.isin([\"2_og\"])], df_synthesized[df_synthesized.group_id.isin([\"2_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)\n\n\n\n\n\n\n\ndf_vis = pd.concat([df[df.group_id.isin([\"3_og\"])], df_synthesized[df_synthesized.group_id.isin([\"3_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)\n\n\n\n\n\n\n\ndf_vis = pd.concat([df[df.group_id.isin([\"4_og\"])], df_synthesized[df_synthesized.group_id.isin([\"4_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "shriyan.io",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My current work and interest involves tabular data and computer vision problems."
  }
]