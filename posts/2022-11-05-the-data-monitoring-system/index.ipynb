{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset\n",
    "> The Data You Know; The Story You Donâ€™t\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blogpost I will talk about a data monitoring system that I built to monitor the quality and availibility in real time. The system has an `Airflow` to schedule jobs, `PostgreSQL` to store data and `Superset` to visualize the data and monitor its quality.\n",
    "\n",
    "Through this system I monitored the data quality, data consistency, data drift. This system enables you to take actions like - identifying discrepancies in ETL pipeline if expected data is missing, anomaly in the data that is causing the business objective to fail and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am assuming that you have Docker installed in your system. If not install it first and follow the next steps below.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Install Airflow, PostgreSQL and Superset with Docker.\n",
    "\n",
    "2. Fetch data, preprocess and push to PostgreSQL using Airflow.\n",
    "\n",
    "3. Visualize and monitor the data in Superset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the high level architecture look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](monitoring-system.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up PostgreSQL, Airflow and Superset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Docker PostgreSQL\n",
    "```\n",
    "docker pull postgres\n",
    "docker run --name <name_of_container> -e POSTGRES_USER=<username> -e POSTGRES_PASSWORD=<password> -p 5432:5432 -v /data:/var/lib/postgresql/data -d <db_name>\n",
    "```\n",
    "\n",
    "### 2. Docker Airflow\n",
    "\n",
    "Run the below instructions inside the repo where your your python code resides.\n",
    "```\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.4.1/docker-compose.yaml'\n",
    "mkdir -p ./dags ./logs ./plugins\n",
    "echo -e 'AIRFLOW_UID=$(id -u)' > .env\n",
    "docker-compose up airflow-init\n",
    "```\n",
    "\n",
    "### 3. Docker Superset\n",
    "```\n",
    "git clone https://github.com/apache/superset.git\n",
    "cd superset\n",
    "docker-compose -f docker-compose-non-dev.yml pull\n",
    "docker-compose -f docker-compose-non-dev.yml up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you go about scheduling a task using `Airflow`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to official documentation:\n",
    "> Apache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows.\n",
    "\n",
    "The below steps ensure that you have an up and running job scheduled at regular intervals:\n",
    "\n",
    "1. Define a function to fetch data from the API or from S3 or any other source\n",
    "\n",
    "```\n",
    "def download_api_data():\n",
    "    print(f\"Fetching data....\")\n",
    "    response = requests.get(<API url>)\n",
    "    data = response.json()\n",
    "    print(f\"Total number of data: {len(data)}\")\n",
    "    json_object = json.dumps(data, indent=2)\n",
    "    with open(f\"/tmp/pdl_{currency}_hourly.json\", \"w\") as f:\n",
    "        f.write(json_object)\n",
    "    print(f\"Finished downloading data.....\")\n",
    "```\n",
    "\n",
    "The above function fetches data from the API and then stores it as json file for further processing.\n",
    "\n",
    "2. Define a function to move downloaded data(json, csv etc.) to PostgreSQL\n",
    "\n",
    "```\n",
    "def move_pdl_data_to_postgres(**kwargs):\n",
    "    currency = kwargs[\"currency\"]\n",
    "    print(f\"Populating for {currency} has started\")\n",
    "    with open(f\"/tmp/pdl_{currency}_hourly.json\") as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Define your preprocessing steps here like typecasting a column according to the Postgresql schema and any other steps specific to your use case\n",
    "\n",
    "    print(\"All values created, starting the push to db\")\n",
    "    df.to_sql(name=\"<name-of-sql-table>\", con=engine, index=False, if_exists=\"append\", chunksize=300)\n",
    "```\n",
    "\n",
    "In the above function we load the json data downloaded in step 1 inside a dataframe and then move it to PostgreSQL table, defined in our `con` parameter of `to_sql` function.\n",
    "\n",
    "3. Define a `DAG`\n",
    "\n",
    "- DAG object is needed to define how we are going to schedule our various tasks. \n",
    "- Here we pass a string that defines the dag_id, which serves as a unique identifier for your DAG and also description. \n",
    "- We also schedule a DAG using `schedule_interval` parameter to run it at hourly frequency and also provide start_date of the DAG.\n",
    "- Below is an example of the DAG definition:\n",
    "\n",
    "```\n",
    "dag = DAG(\n",
    "    dag_id=\"data-migration\",\n",
    "    description=\"Daily data monitoring pipeline\",\n",
    "    schedule_interval=\"0 * * * *\",\n",
    "    start_date=datetime(2022, 10, 17),\n",
    ")\n",
    "```\n",
    "\n",
    "4. Define `PythonOperator`\n",
    "\n",
    "- Operators are tasks that define a unit of work to be done.\n",
    "- There are manny different kind of operators that you can play around with in Airflow. But we will stick to `PythonOperator`, which takes python function as a parameter.\n",
    "- Here we define the `task_id`, `python_callable` and above defined `dag` object.\n",
    "- Below is how we define the `PythonOperator` object\n",
    "\n",
    "```\n",
    "PythonOperator(\n",
    "            task_id=f\"download_json\",\n",
    "            python_callable=download_json,\n",
    "            dag=dag,\n",
    "        )\n",
    "```\n",
    "\n",
    "5. Setup task dependencies\n",
    "\n",
    "Lets say we have 2 PythonOperator defined as 2 tasks and one task is dependent on the other. In our case we first fetch the data from API and then push the data to PostgreSQL. So setting up task dependency kind of becomes and it is defined by using `>>` operator as follows:\n",
    "\n",
    "```\n",
    "task1 >> task2\n",
    "```\n",
    "\n",
    "Here the Airflow DAG knows that it has to first finish running the `task1` and then move on to `task2`. Any failure in task1 will result in termination of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Airflow UI going to look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](airflow-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above UI can be accessed after Airflow login and navigating as follows: `<DAG-name> > Graph`.\n",
    "\n",
    "The Graph shows you the various tasks that are scheduled to run and each row defines multiple tasks and how each one is dependent on the other i.e Task `move_pdl_data_to_postgres_ADA` is dependent on `download_pdl_json_ADA` and hence has to be completed first.\n",
    "\n",
    "The subsequent rows follow a similar pattern and here we have demonstrated multiple different jobs scheduled inside a single DAG, where each job does the same thing as other, but for different type of data i.e for different bitcoin currencies in our scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to visualize the raw data in `Apache Superset`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](superset-dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Superset is a data exploration and visualization platform and we are going to leverage it to use it as our frontend for monitoring the data we move to the PostgreSQL at regular intervals.\n",
    "- As seen in the above example dashboard we are doing some sanity check and checking the trend for a bitcoin currency.\n",
    "- So playing around with visualizations specific to your data and problem statement is straight forward in Superset and it comes with a bunch of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task can be further expanded in various aspects each from PostgreSQl, Airflow and Superset perspective, by adding more sources of information that we want to monitor in real time and keep adding more tables to our PostgreSQL database, schedule more DAGs in our Airflow container and add more dashboards monitoring the nature of different data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
